---
title: "Практическая работа 004"
author: "dolgov18012005@yandex.ru"
format: 
  md:
    output-file: README.md
---

## Цель работы

1. Зекрепить практические навыки использования языка программирования R для обработки данных
2. Закрепить знания основных функций обработки данных экосистемы tidyverse языка R
3. Закрепить навыки исследования метаданных DNS трафика

## Исходные данные

1. Программное обеспечение Manjaro
2. Rstudio Desktop
3. Интерпретатор языка R 4.5.1

## Задание

Используя программный пакет dplyr, освоить анализ DNS логов с помощью языка программирования R.

## Ход работы

1. Подготовка данных \
    1.1. Импортируйте данные DNS – https://storage.yandexcloud.net/dataset.ctfsec/dns.zip \
    1.2. Добавьте пропущенные данные о структуре данных (назначении столбцов) \
    1.3. Преобразуйте данные в столбцах в нужный формат \
    1.4. Просмотрите общую структуру данных с помощью функции glimpse() \

2. Анализ \
    2.1. Сколько участников информационного обмена в сети Доброй Организации? \
    2.2. Какое соотношение участников обмена внутри сети и участников обращений к внешним ресурсам? \
    2.3. Найдите топ-10 участников сети, проявляющих наибольшую сетевую активность. \
    2.4. Найдите топ-10 доменов, к которым обращаются пользователи сети и соответственное количество обращений \
    2.5. Опеределите базовые статистические характеристики (функция summary() ) интервала времени между последовательными обращениями к топ-10 доменам. \
    2.6. Часто вредоносное программное обеспечение использует DNS канал в качестве канала управления, периодически отправляя запросы на подконтрольный злоумышленникам DNS сервер. По периодическим запросам на один и тот же домен можно выявить скрытый DNS канал. Есть ли такие IP адреса в исследуемом датасете? \
3. Обогащение данных \
    3.1. Определите местоположение (страну, город) и организацию-провайдера для топ-10 доменов. Для этого можно использовать сторонние сервисы, например http://ip-api.com (API-эндпоинт – http://ip-api.com/json).


### Шаг 1

Для начала подключим необходимые библиотеки

```{r}
library(dplyr, readr)
library(lubridate)
library(tidyverse)
```

#### Импорт данных

```{r}
dns_data <- read.csv(file = 'dns.log', header = FALSE, sep = '\t', na.strings = c("-"))
```

#### Добавление пропущенных данных о струткуре столбцов

```{r}
colnames(dns_data) <- c(
  "ts", "uid", "id.orig_h", "id.orig_p", "id.resp_h", "id.resp_p",
  "proto", "trans_id", "query", "qclass", "qclass_name", "qtype",
  "qtype_name", "rcode", "rcode_name", "AA", "TC", "RD", "RA", "Z",
  "answers", "TTLs", "rejected"
)
```

#### Преобразование данных в нужный формат

```{r}
dns_data <- dns_data %>%
  mutate(
    ts = as_datetime(ts),
    across(c(id.orig_p, id.resp_p, trans_id), as.integer)

  )
```

#### Общая структура

```{r}
glimpse(dns_data)
```

### Шаг 2

#### Кол-во участников информационного обмена в сети Доброй орагнизации

```{r}
length(unique(c(dns_data$id.orig_h, dns_data$id.resp_h)))
```

#### Соотношение участников обмена внутри сети и участников обращений к внешним ресурсам

```{r}
internal_patterns <- c("10\\.", "192\\.168\\.", "172\\.(1[6-9]|2[0-9]|3[0-1])\\.")

amonut_external_users <- dns_data %>% 
  filter(., !grepl(paste(internal_patterns, collapse = "|"), id.resp_h)) %>%
  pull(id.orig_h) %>%
  unique() %>%
  length()



amount_internal_users = length(unique(c(dns_data$id.orig_h, dns_data$id.resp_h))) - amonut_external_users 
amount_internal_users / amonut_external_users
```

#### Топ-10 участников с наибольшей сетевой активностью

```{r}
dns_data %>% group_by(., id.orig_h) %>% count(sort=TRUE) %>% head(10)
```

#### Топ-10 доменов, к которым обращаются пользователи

```{r}
top_domains <- dns_data %>% count(query, sort = TRUE) %>% head(10)

top_domains
```

#### Статистика по топ доменам

```{r}
top10_list <- top_domains$query

dns_top10 <- dns_data %>%
  filter(., query %in% top10_list) %>%
  arrange(., query, ts)

intervals_by_domain <- dns_top10 %>%
  group_by(query) %>%
  mutate(
    time_diff_sec = c(NA, diff(ts))
  ) %>%
  ungroup()

summary_stats <- intervals_by_domain %>%
  group_by(query) %>%
  summarise(
    summary_text = list(summary(time_diff_sec, na.rm = TRUE)),
    .groups = "drop"
  )

for (i in seq_len(nrow(summary_stats))) {
  cat("\nДомен:", summary_stats$query[i], "\n")
  print(summary_stats$summary_text[[i]])
}

```
### Шаг 3

#### Подозрительные ip адреса

```{r}

top_domains <- dns_data %>%
  count(query, sort = TRUE) %>%
  filter(n > 5) %>%
  pull(query)

get_timing_stats <- function(domain_data) {
  times <- sort(domain_data$ts)
  diffs <- as.numeric(diff(times))
  tibble(
    query = domain_data$query[1],
    n_intervals = length(diffs),
    mean_gap = mean(diffs),
    sd_gap = sd(diffs),
    cv = sd_gap / mean_gap
  )
}

timing_summary <- dns_data %>%
  filter(query %in% top_domains) %>%
  split(.$query) %>%
  map_dfr(~ get_timing_stats(.x)) %>%
  filter(cv < 0.3) %>%
  arrange(sd_gap)

timing_summary
```

#### Определение местоположения и провайдера для топ доменов

```{r}
library(httr)
library(jsonlite)

top_domains <- dns_data %>% count(query, sort = TRUE) %>% head(10)
top10_list <- top_domains$query

get_geo_info <- function(domain) {
  url <- paste0("http://ip-api.com/json/", URLencode(domain))
  resp <- GET(url)
  
  if (http_type(resp) != "application/json") {
    return(tibble(domain = domain, country = NA, city = NA, isp = NA, success = FALSE))
  }
  
  data <- content(resp, "parsed", encoding = "UTF-8")
  
  if (is.null(data) || !isTRUE(data$status == "success")) {
    return(tibble(domain = domain, country = NA, city = NA, isp = NA, success = FALSE))
  }
  
  tibble(
    domain = domain,
    country = data$country %||% NA,
    city = data$city %||% NA,
    isp = data$isp %||% NA,
    success = TRUE
  )
}

geo_results <- tibble(domain = top10_list) %>%
  pmap_dfr(~ get_geo_info(.x))

print(geo_results %>% select(domain, country, city, isp))

```

### Итог

Отчёт написан и оформлен


